import torch.nn as nn  # å¯¼å…¥ PyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å—,æä¾›äº†æ„å»ºç¥ç»ç½‘ç»œæ‰€éœ€çš„åŠŸèƒ½
import sys  # å¯¼å…¥ç³»ç»Ÿæ¨¡å—,ç”¨äºæ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½
sys.path.append('..')  # å°†ä¸Šçº§ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­,ä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—
from lib.utils import *  # ä» lib.utils æ¨¡å—ä¸­å¯¼å…¥æ‰€æœ‰åŠŸèƒ½,ç”¨äºåç»­ä»£ç ä¸­çš„å·¥å…·å‡½æ•°
# from pygcn.layers import GraphConvolution
import torch
import torch.nn as nn
import torch.nn.functional as F

import logging

# å¼•ç”¨ä¸»ç¨‹åºä¸­çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


# å®šä¹‰ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ï¼ˆMLPï¼‰
"""
å¤šå±‚æ„ŸçŸ¥æœº(MLP,Multilayer Perceptron)ä¹Ÿå«äººå·¥ç¥ç»ç½‘ç»œ(ANN,Artificial Neural Network)
Figure3ä¸­çš„Feature Extraction(æå–ç‰¹å¾)
***è¾“å…¥æ—¶é—´åºåˆ—æ•°æ®é€šè¿‡MLPç±»è¿›è¡Œç‰¹å¾æå–***
å¤šä¸ªæ—¶é—´åºåˆ—Sé€šè¿‡ç‰¹å¾æå–è½¬æ¢ä¸ºéšå«è¡¨ç¤ºh

ç›¸å…³ä»£ç ï¼š
X = self.mlp1(inputs)  # ä½¿ç”¨ MLP æå–ç‰¹å¾Graph_learnerç±»ä¸­çš„forwardå‡½æ•°
"""
class MLP(nn.Module):
    """Two-layer fully-connected ELU net with batch norm."""
    # å®šä¹‰ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªå…¨è¿æ¥å±‚ã€ELU(æŒ‡æ•°çº¿æ€§å•å…ƒ) æ¿€æ´»å‡½æ•°å’Œæ‰¹é‡å½’ä¸€åŒ–çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹

    def __init__(self, n_in, n_hid, n_out, do_prob=0.):
        """
        åˆå§‹åŒ– MLP æ¨¡å‹
        :param n_in: è¾“å…¥å±‚çš„èŠ‚ç‚¹æ•°
        :param n_hid: éšè—å±‚çš„èŠ‚ç‚¹æ•°
        :param n_out: è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°
        :param do_prob: dropout çš„æ¦‚ç‡,é»˜è®¤å€¼ä¸º 0(å³ä¸ä½¿ç”¨ dropout)
        """
        super(MLP, self).__init__()  # è°ƒç”¨çˆ¶ç±»ï¼ˆnn.Moduleï¼‰çš„åˆå§‹åŒ–å‡½æ•°
        self.fc1 = nn.Linear(n_in, n_hid)  # å®šä¹‰ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚,è¾“å…¥ç»´åº¦ä¸º n_in,è¾“å‡ºç»´åº¦ä¸º n_hid
        self.fc2 = nn.Linear(n_hid, n_out)  # å®šä¹‰ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚,è¾“å…¥ç»´åº¦ä¸º n_hid,è¾“å‡ºç»´åº¦ä¸º n_out
        # æ‰¹é‡å½’ä¸€åŒ–å±‚å¯ä»¥å¯¹æ¯ä¸ªå°æ‰¹é‡çš„æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†,æ•°æ®çš„å‡å€¼è°ƒæ•´ä¸º0,æ–¹å·®è°ƒæ•´ä¸º1
        self.bn = nn.BatchNorm1d(n_out)  # å®šä¹‰æ‰¹é‡å½’ä¸€åŒ–å±‚,ç”¨äºè§„èŒƒåŒ–è¾“å‡ºå±‚çš„è¾“å‡º
        self.dropout_prob = do_prob  # ä¿å­˜ dropout æ¦‚ç‡
        # dropout æ˜¯ä¸€ç§é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚
        # åŸºæœ¬æ€æƒ³ï¼šåœ¨æ¯æ¬¡è®­ç»ƒè¿‡ç¨‹ä¸­,éšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒ,è¿«ä½¿ç¥ç»ç½‘ç»œä¸ä¾èµ–æŸäº›ç‰¹å®šçš„èŠ‚ç‚¹å’Œè·¯å¾„,å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

        self.init_weights()  # è°ƒç”¨åˆå§‹åŒ–æƒé‡çš„æ–¹æ³•

    def init_weights(self):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚å’Œæ‰¹é‡å½’ä¸€åŒ–å±‚çš„æƒé‡å’Œåç½®
        """
        for m in self.modules():  # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰å­æ¨¡å—
            if isinstance(m, nn.Linear):  # å¦‚æœå­æ¨¡å—æ˜¯å…¨è¿æ¥å±‚
                nn.init.xavier_normal_(m.weight.data)  # ä½¿ç”¨ Xavier æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡
                m.bias.data.fill_(0.1)  # å°†åç½®åˆå§‹åŒ–ä¸º 0.1
            elif isinstance(m, nn.BatchNorm1d):  # å¦‚æœå­æ¨¡å—æ˜¯æ‰¹é‡å½’ä¸€åŒ–å±‚
                m.weight.data.fill_(1)  # å°†æƒé‡åˆå§‹åŒ–ä¸º 1
                m.bias.data.zero_()  # å°†åç½®åˆå§‹åŒ–ä¸º 0

    def batch_norm(self, inputs):
        """
        æ‰¹é‡å½’ä¸€åŒ–
        :param inputs: è¾“å…¥æ•°æ®
        :return: æ‰¹é‡å½’ä¸€åŒ–åçš„æ•°æ®
        """
        x = inputs.view(inputs.size(0) * inputs.size(1), -1)  # å°†è¾“å…¥æ•°æ®é‡å¡‘ä¸ºäºŒç»´å½¢å¼
        x = self.bn(x)  # åº”ç”¨æ‰¹é‡å½’ä¸€åŒ–
        return x.view(inputs.size(0), inputs.size(1), -1)  # å°†æ•°æ®é‡å¡‘å›åŸå§‹çš„ä¸‰ç»´å½¢å¼

    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­
        :param inputs: è¾“å…¥æ•°æ®[B,N,Fin]
        :return: æ¨¡å‹çš„è¾“å‡º
        """
        x = F.elu(self.fc1(inputs))  # é€šè¿‡ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚,å¹¶ä½¿ç”¨ ELU æ¿€æ´»å‡½æ•°
        x = F.dropout(x, self.dropout_prob, training=self.training)  # åº”ç”¨ dropout,éšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºç½®0,ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
        x = F.elu(self.fc2(x))  # é€šè¿‡ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚,å¹¶ä½¿ç”¨ ELU æ¿€æ´»å‡½æ•°
        return self.batch_norm(x)  # è¿”å›æ‰¹é‡å½’ä¸€åŒ–åçš„è¾“å‡º [B,N,Fout]

# å®šä¹‰ä¸€ä¸ªå›¾å­¦ä¹ æ¨¡å‹
"""
Figure3ä¸­çš„Relation Inference(å…³ç³»æ¨æ–­)éƒ¨åˆ†,æ˜¾ç¤ºäº†å¦‚ä½•ä»æå–çš„ç‰¹å¾hæ¨æ–­å‡ºèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ğœƒ

åœ¨è¾“å…¥æ•°æ®ä¸­å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»
1.ç‰¹å¾æå–:é€šè¿‡MLPæ¨¡å‹å¯¹è¾“å…¥æ•°æ®è¿›è¡Œç‰¹å¾æå–,å°†å…¶è½¬æ¢ä¸ºæ›´é«˜ç»´çš„éšå«è¡¨ç¤ºã€‚   ä»£ç ä¸­çš„ self.mlp1 éƒ¨åˆ†
2.è®¡ç®—æŸ¥è¯¢å’Œé”®:é€šè¿‡'wq'å’Œ'wk'çº¿æ€§å±‚è®¡ç®—æŸ¥è¯¢å’Œé”®å‘é‡ã€‚   ä»£ç ä¸­çš„ self.Wq(X) å’Œ self.Wk(X) éƒ¨åˆ†ã€‚
3.å…³ç³»æ¨æ–­:é€šè¿‡æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡çš„ç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚
é€šè¿‡å‘é‡ä¹‹é—´çš„ç‚¹ç§¯æ¥æ¨æ–­èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚ç‚¹ç§¯ç»“æœè¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦å’Œå…³è”ç¨‹åº¦,å³æ³¨æ„åŠ›æƒé‡ã€‚
ä»£ç ä¸­çš„ torch.matmul(Xq, Xk.transpose(-1, -2)) éƒ¨åˆ†,å¯¹åº”å›¾ä¸­çš„relation inferenceå…³ç³»æ¨æ–­éƒ¨åˆ†ã€‚
"""
class Graph_learner(nn.Module):
    def __init__(self, n_in, n_hid, n_head_dim, head, do_prob=0.):  # n_in = T
        """
        åˆå§‹åŒ–å›¾å­¦ä¹ æ¨¡å‹
        :param n_in: è¾“å…¥ç»´åº¦ï¼ˆç‰¹å¾æ•°é‡ T)
        :param n_hid: éšè—å±‚ç»´åº¦ï¼ˆç”¨äº MLP çš„éšè—å±‚å¤§å°ï¼‰
        :param n_head_dim: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
        :param head: æ³¨æ„åŠ›å¤´çš„æ•°é‡
        :param do_prob: dropout çš„æ¦‚ç‡ï¼ˆé»˜è®¤å€¼ä¸º 0)
        """
        super(Graph_learner, self).__init__()
        self.n_hid = n_hid  # éšè—å±‚çš„ç»´åº¦
        self.head = head  # å¤´çš„æ•°é‡
        self.n_in = n_in  # è¾“å…¥ç»´åº¦
        self.n_head_dim = n_head_dim  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # å®šä¹‰ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç”¨äºç‰¹å¾æå–
        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)  # å®šä¹‰ä¸€ä¸ª MLP æ¨¡å‹,ç”¨äºå¤„ç†è¾“å…¥æ•°æ®
        
        # Wq å’Œ Wk æ˜¯ç”¨äºè®¡ç®—æŸ¥è¯¢ï¼ˆQueryï¼‰å’Œé”®ï¼ˆKeyï¼‰çš„çº¿æ€§å˜æ¢å±‚
        # è¾“å‡ºç»´åº¦ n_head_dim * head,å³å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­æ¯ä¸ªå¤´çš„ç»´åº¦ä¹˜ä»¥å¤´çš„æ•°é‡
        self.Wq = nn.Linear(n_hid, n_head_dim * head)  # å®šä¹‰æŸ¥è¯¢æƒé‡çŸ©é˜µ,ç»´åº¦ä¸º n_hid åˆ° n_head_dim * head
        self.Wk = nn.Linear(n_hid, n_head_dim * head)  # å®šä¹‰é”®æƒé‡çŸ©é˜µ,ç»´åº¦ä¸º n_hid åˆ° n_head_dim * head

        for m in [self.Wq, self.Wk]:  # å¯¹æƒé‡çŸ©é˜µè¿›è¡Œåˆå§‹åŒ–
            if isinstance(m, nn.Linear):  # å¦‚æœå­æ¨¡å—æ˜¯å…¨è¿æ¥å±‚
                nn.init.xavier_normal_(m.weight.data)  # ä½¿ç”¨ Xavier æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡
                m.bias.data.fill_(0.1)  # å°†åç½®åˆå§‹åŒ–ä¸º 0.1

    def forward(self, inputs):  # inputs: [B, N, T(features)]
        """
        å‰å‘ä¼ æ’­
        :param inputs: è¾“å…¥æ•°æ®,å½¢çŠ¶ä¸º [B, N, T]ï¼ˆæ‰¹é‡å¤§å°,èŠ‚ç‚¹æ•°é‡,ç‰¹å¾æ•°é‡ï¼‰
        :return: æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
        """
        X = self.mlp1(inputs)  # é€šè¿‡ MLP æ¨¡å‹å¤„ç†è¾“å…¥æ•°æ®
        Xq = self.Wq(X)  # è®¡ç®—æŸ¥è¯¢å‘é‡
        Xk = self.Wk(X)  # è®¡ç®—é”®å‘é‡

        # è·å–è¾“å…¥çš„ç»´åº¦ä¿¡æ¯
        B, N, n_hid = Xq.shape  # è·å–æ‰¹é‡å¤§å° B,èŠ‚ç‚¹æ•°é‡ N,éšè—å±‚ç»´åº¦ n_hid

        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        # è°ƒæ•´æŸ¥è¯¢å’Œé”®å‘é‡çš„å½¢çŠ¶ä»¥é€‚åº”å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        # å°†æŸ¥è¯¢å’Œé”®çŸ©é˜µé‡å¡‘,ä»¥ä¾¿å°†å¤´çš„ç»´åº¦åˆ†ç¦»å‡ºæ¥
        Xq = Xq.view(B, N, self.head, self.n_head_dim)  # é‡å¡‘æŸ¥è¯¢å‘é‡,å½¢çŠ¶ä¸º [B, N, head, head_dim]
        Xk = Xk.view(B, N, self.head, self.n_head_dim)  # é‡å¡‘é”®å‘é‡,å½¢çŠ¶ä¸º [B, N, head, head_dim]

        # è°ƒæ•´ç»´åº¦é¡ºåº,ä¾¿äºåç»­çš„çŸ©é˜µä¹˜æ³•æ“ä½œ
        Xq = Xq.permute(0, 2, 1, 3)  # è°ƒæ•´ç»´åº¦é¡ºåº,å½¢çŠ¶ä¸º [B, head, N, head_dim]
        Xk = Xk.permute(0, 2, 1, 3)  # è°ƒæ•´ç»´åº¦é¡ºåº,å½¢çŠ¶ä¸º [B, head, N, head_dim]

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡çŸ©é˜µ,ä½¿ç”¨çŸ©é˜µä¹˜æ³•å°†æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡ç›¸ä¹˜,å¹¶å¯¹æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬ç½®
        probs = torch.matmul(Xq, Xk.transpose(-1, -2))  # è®¡ç®—æ³¨æ„åŠ›æƒé‡çŸ©é˜µ    Relation Inference,Figure3ä¸­çš„å…³ç³»æ¨æ–­
        return probs # è¿”å›æ³¨æ„åŠ›æƒé‡çŸ©é˜µ,æ¯ä¸ªå¤´ä¸Š,å„èŠ‚ç‚¹ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µ[B,H,N,N]
        # probsè¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„ç›¸å…³æ€§,å¯ä»¥ç”¨äºç”Ÿæˆé‚»æ¥çŸ©é˜µ

# # GCNå®ç°1
# class GCNCell(torch.nn.Module):
#     def __init__(self, device, num_units, max_diffusion_step, num_nodes, 
#                 filter_type="laplacian",nonlinearity='tanh'):
#         super().__init__()
#         self._activation = torch.tanh if nonlinearity == 'tanh' else torch.relu
#         self.device = device
#         self._num_nodes = num_nodes
#         self._num_units = num_units

#         self._gconv = nn.Linear(self._num_units, self._num_units)

#         for m in self.modules():
#             if isinstance(m, nn.Linear):
#                 nn.init.xavier_normal_(m.weight.data)
#                 m.bias.data.fill_(0.1)

#     def _calculate_random_walk_matrix(self, adj_mx):
#         adj_mx = adj_mx + torch.eye(int(adj_mx.shape[0])).to(self.device)
#         d = torch.sum(adj_mx, 1)
#         d_inv = 1. / d
#         d_inv = torch.where(torch.isinf(d_inv), torch.zeros(d_inv.shape).to(self.device), d_inv)
#         d_mat_inv = torch.diag(d_inv)
#         random_walk_mx = torch.mm(d_mat_inv, adj_mx)
#         return random_walk_mx

#     def forward(self, inputs, hx,adj):
#         logging.info(f"è¾“å…¥inputså½¢çŠ¶ä¸º{inputs.shape}")  # [128, 51, 64]
#         logging.info(f"é‚»æ¥çŸ©é˜µadjå½¢çŠ¶ä¸º{adj.shape}")  # [128, 51, 51]

#         # è¾“å…¥é‡å¡‘
#         B = inputs.shape[0]
#         inputs = inputs.reshape(B, self._num_nodes, -1)  # [128, 51, 64]

#         # è®¡ç®—éšæœºæ¸¸èµ°çŸ©é˜µ
#         random_walk_mx = self._calculate_random_walk_matrix(adj[0])  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„é‚»æ¥çŸ©é˜µ
#         random_walk_mx = random_walk_mx.unsqueeze(0).repeat(B, 1, 1)  # [128, 51, 51]

#         # å›¾å·ç§¯è®¡ç®—
#         gconv_output = self._gconv(inputs)  # [128, 51, 64]
#         if self._activation is not None:
#             gconv_output = self._activation(gconv_output)

#         return gconv_output.reshape(B, -1)  # [128, 3264]


class GCNCell(torch.nn.Module):
    def __init__(self, device, num_units, max_diffusion_step, num_nodes, 
                filter_type="laplacian",nonlinearity='tanh'):
        super().__init__()
        self._activation = torch.tanh if nonlinearity == 'tanh' else torch.relu
        self.device = device
        self._num_nodes = num_nodes
        self._num_units = num_units

        self._gconv = nn.Linear(self._num_units, self._num_units)

        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
                m.bias.data.fill_(0.1)

    def _calculate_random_walk_matrix(self, adj_mx):
        adj_mx = adj_mx + torch.eye(int(adj_mx.shape[0])).to(self.device)
        d = torch.sum(adj_mx, 1)
        d_inv = 1. / d
        d_inv = torch.where(torch.isinf(d_inv), torch.zeros(d_inv.shape).to(self.device), d_inv)
        d_mat_inv = torch.diag(d_inv)
        random_walk_mx = torch.mm(d_mat_inv, adj_mx)
        return random_walk_mx

    def forward(self, inputs, hj,adj):
        # logging.info(f"GCNCellè¾“å…¥inputså½¢çŠ¶ä¸º{inputs.shape}")  # [128, 51, 64]
        # logging.info(f"é‚»æ¥çŸ©é˜µadjå½¢çŠ¶ä¸º{adj.shape}")  # [128, 51, 51]

        B = inputs.shape[0]
        inputs = inputs.reshape(B, self._num_nodes, -1)  # [128, 51, 64]

        # è®¡ç®—éšæœºæ¸¸èµ°çŸ©é˜µ
        random_walk_mx = self._calculate_random_walk_matrix(adj[0])  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„é‚»æ¥çŸ©é˜µ
        random_walk_mx = random_walk_mx.unsqueeze(0).repeat(B, 1, 1)  # [128, 51, 51]

        # ä½¿ç”¨å›¾å·ç§¯è¿›è¡Œä¿¡æ¯ä¼ æ’­
        gconv_output = torch.bmm(random_walk_mx, inputs)  # [128, 51, 64]
        
        # åº”ç”¨æ¿€æ´»å‡½æ•°
        if self._activation is not None:
            gconv_output = self._activation(gconv_output)

        return gconv_output.reshape(B, -1)  # [128, 3264]



# å®šä¹‰ç¼–ç å™¨æ¨¡å‹
"""
å®ç°äº†ä¸€ä¸ªåŸºäºå¤šå±‚æ‰©æ•£å·ç§¯é—¨æ§å¾ªç¯å•å…ƒ(DCGRU)çš„ç¼–ç å™¨æ¨¡å‹,ä¸»è¦ç”¨äºå¤„ç†æ—¶é—´åºåˆ—æ•°æ®å’Œå›¾ç»“æ„æ•°æ®ã€‚
å°†è¾“å…¥æ•°æ®ä¸å›¾çš„é‚»æ¥çŸ©é˜µç›¸ç»“åˆ,å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»,é€šè¿‡å¾ªç¯ç½‘ç»œå±‚è¿›è¡Œæ—¶é—´åºåˆ—å»ºæ¨¡ã€‚
é€šè¿‡å †å å¤šå±‚DCGRUCell_,å®ç°äº†å¯¹å›¾ç»“æ„æ—¶åºæ•°æ®çš„ç¼–ç 
"""
class EncoderModel(nn.Module):
    def __init__(self, device, n_dim, n_hid, max_diffusion_step, num_nodes, num_rnn_layers, filter_type):
        """
        åˆå§‹åŒ–ç¼–ç å™¨æ¨¡å‹
        :param device: è®¾å¤‡(CPU æˆ– GPU)
        :param n_dim: è¾“å…¥ç»´åº¦
        :param n_hid: éšè—å±‚ç»´åº¦,æ¯ä¸ªDCGRUçš„éšè—å±‚å•å…ƒæ•°é‡Fhid
        :param max_diffusion_step: æœ€å¤§æ‰©æ•£æ­¥æ•°
        :param num_nodes: èŠ‚ç‚¹æ•°é‡
        :param num_rnn_layers: RNN å±‚æ•°
        :param filter_type: æ»¤æ³¢å™¨ç±»å‹
        """
        super(EncoderModel, self).__init__()
        self.device = device  # è®¾å¤‡
        self.input_dim = n_dim  # è¾“å…¥ç»´åº¦
        self.rnn_units = n_hid  # éšè—å±‚ç»´åº¦,å³æ¯ä¸ªDCGRUçš„éšè—æ•°é‡å•å…ƒFhid
        self.max_diffusion_step = max_diffusion_step  # æœ€å¤§æ‰©æ•£æ­¥æ•°
        self.num_nodes = num_nodes  # èŠ‚ç‚¹æ•°é‡
        self.num_rnn_layers = num_rnn_layers  # DCGRUå±‚çš„æ•°é‡
        self.filter_type = filter_type  # æ»¤æ³¢å™¨ç±»å‹
        # # å®šä¹‰æ¯å±‚çš„éšè—çŠ¶æ€å¤§å°ï¼šèŠ‚ç‚¹æ•° * éšè—å•å…ƒæ•°
        self.hidden_state_size = self.num_nodes * self.rnn_units  # éšè—çŠ¶æ€å¤§å°
        # åˆ›å»ºäº†num_rnn_layersä¸ªDCGRUå•å…ƒ,æ¯å±‚ç”¨äºå¤„ç†è¾“å…¥æ•°æ®å’Œé‚»æ¥çŸ©é˜µ,å¹¶é€å±‚é€’å½’æ›´æ–°éšè—çŠ¶æ€
        self.dcgru_layers = nn.ModuleList(
            [GCNCell(self.device, self.rnn_units, self.max_diffusion_step, self.num_nodes,
                       filter_type=self.filter_type) for _ in range(self.num_rnn_layers)])  # å®šä¹‰å¤šä¸ª DCGRU å•å…ƒ

    def forward(self, inputs, adj, hidden_state=None):
        """
        å‰å‘ä¼ æ’­
        :param inputs: è¾“å…¥æ•°æ®[B,N*Fin]
        :param adj: é‚»æ¥çŸ©é˜µ[B,N,N]
        :param hidden_state: éšè—çŠ¶æ€
        :return: è¾“å‡ºå’Œéšè—çŠ¶æ€
        """
        batch_size = inputs.shape[0]  # è·å–æ‰¹é‡å¤§å°
        if hidden_state is None:
            # å¦‚æœæ²¡æœ‰æä¾›éšè—çŠ¶æ€,åˆ™åˆå§‹åŒ–ä¸ºå…¨é›¶
            hidden_state = torch.zeros((self.num_rnn_layers, batch_size, self.hidden_state_size)).to(self.device)  # åˆå§‹åŒ–éšè—çŠ¶æ€
        hidden_states = []  # ç”¨äºå­˜å‚¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€
        output = inputs  # è¾“å…¥æ•°æ®ä½œä¸ºåˆå§‹çš„è¾“å‡º

        # é€å±‚å¤„ç†æ•°æ®
        for layer_num, dcgru_layer in enumerate(self.dcgru_layers):
            # è°ƒç”¨æ¯ä¸€å±‚ DCGRU,è®¡ç®—è¾“å‡ºå’Œæ›´æ–°éšè—çŠ¶æ€
            next_hidden_state = dcgru_layer(output, hidden_state[layer_num], adj)  # è®¡ç®—ä¸‹ä¸€ä¸ªéšè—çŠ¶æ€
            hidden_states.append(next_hidden_state)  # å°†å½“å‰å±‚çš„éšè—çŠ¶æ€ä¿å­˜åˆ°hidden_staesåˆ—è¡¨ä¸­
            output = next_hidden_state  # å°†å½“å‰å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥

        # è¿”å›æœ€åä¸€å±‚çš„è¾“å‡ºå’Œæ‰€æœ‰éšè—çŠ¶æ€
        # output[B,N*Fhid] hidden_states [L,B,N*Fhid]
        return output, torch.stack(hidden_states)

# å®šä¹‰ GRELEN æ¨¡å‹
class Grelen(nn.Module):
    """
    GRELEN Model.
    """
    def __init__(self, device, T, target_T, Graph_learner_n_hid, Graph_learner_n_head_dim, Graph_learner_head, temperature,
                 hard, GRU_n_dim, max_diffusion_step, num_nodes, num_rnn_layers, filter_type, do_prob=0.):
        """
        åˆå§‹åŒ– GRELEN æ¨¡å‹
        :param device: è®¾å¤‡(CPU æˆ– GPU)
        :param T: è¾“å…¥åºåˆ—é•¿åº¦
        :param target_T: ç›®æ ‡åºåˆ—é•¿åº¦
        :param Graph_learner_n_hid: å›¾å­¦ä¹ å™¨éšè—å±‚ç»´åº¦
        :param Graph_learner_n_head_dim: å›¾å­¦ä¹ å™¨å¤´ç»´åº¦
        :param Graph_learner_head: å›¾å­¦ä¹ å™¨å¤´æ•°é‡
        :param temperature: Gumbel-softmax æ¸©åº¦å‚æ•°
        :param hard: æ˜¯å¦ä½¿ç”¨ç¡¬ Gumbel-softmax
        :param GRU_n_dim: GRU éšè—å±‚ç»´åº¦
        :param max_diffusion_step: æœ€å¤§æ‰©æ•£æ­¥æ•°
        :param num_nodes: èŠ‚ç‚¹æ•°é‡
        :param num_rnn_layers: RNN å±‚æ•°
        :param filter_type: æ»¤æ³¢å™¨ç±»å‹
        :param do_prob: dropout æ¦‚ç‡
        """
        super(Grelen, self).__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        self.device = device  # è®¾ç½®æ¨¡å‹è¿è¡Œçš„è®¾å¤‡
        self.len_sequence = T  # è®¾ç½®è¾“å…¥åºåˆ—é•¿åº¦
        self.target_T = target_T  # è®¾ç½®é¢„æµ‹çš„ç›®æ ‡åºåˆ—é•¿åº¦

        # åˆå§‹åŒ–å›¾å­¦ä¹ å™¨,è´Ÿè´£å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ï¼ˆRelation Inference éƒ¨åˆ†ï¼‰
        # é€šè¿‡è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„qå’Œkç¡®å®šèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»,ä»è€Œç”Ÿæˆå›¾ç»“æ„çš„æ¦‚ç‡ã€‚
        self.graph_learner = Graph_learner(T, Graph_learner_n_hid, Graph_learner_n_head_dim, Graph_learner_head, do_prob)
        
        # ç”¨äºè¾“å…¥æ—¶åºæ•°æ®æŠ•å½±çš„çº¿æ€§å±‚,æŠ•å½±ç»´åº¦ä¸º GRU çš„éšè—ç»´åº¦
        self.linear1 = nn.Linear(1, GRU_n_dim)
        
        # åˆå§‹åŒ–çº¿æ€§å±‚çš„æƒé‡,ä½¿ç”¨ Xavier æ ‡å‡†æ­£æ€åˆ†å¸ƒ
        nn.init.xavier_normal_(self.linear1.weight.data)
        
        # å°†åç½®åˆå§‹åŒ–ä¸º 0.1
        self.linear1.bias.data.fill_(0.1)

        self.temperature = temperature  # Gumbel-softmax çš„æ¸©åº¦å‚æ•°,ç”¨äºæ§åˆ¶é‡‡æ ·çš„â€œéšæœºæ€§â€
        self.hard = hard  # æ˜¯å¦ä½¿ç”¨ç¡¬ Gumbel-softmax
        self.GRU_n_dim = GRU_n_dim  # GRU éšè—å±‚çš„ç»´åº¦
        self.num_nodes = num_nodes  # å›¾çš„èŠ‚ç‚¹æ•°é‡
        self.head = Graph_learner_head  # å›¾å­¦ä¹ å™¨çš„å¤´æ•°é‡

        # å®šä¹‰å¤šä¸ª EncoderModel æ¨¡å‹,ç”¨äºå¯¹è¾“å…¥æ•°æ®è¿›è¡Œç¼–ç ,æ¯ä¸ªå›¾å­¦ä¹ å¤´éƒ½å¯¹åº”ä¸€ä¸ª EncoderModel
        # ç¼–ç å™¨ä»è¾“å…¥æ—¶åºæ•°æ®ä¸­æå–ç‰¹å¾,ç”±å¤šä¸ªEncoderModelæ¨¡å—ç»„æˆ,å¤„ç†å¤šå¤´æ³¨æ„åŠ›çš„å›¾ç»“æ„ã€‚
        self.encoder_model = nn.ModuleList(
            [EncoderModel(self.device, GRU_n_dim, GRU_n_dim, max_diffusion_step, num_nodes, num_rnn_layers, filter_type)
             for _ in range(self.head - 1)]
        )

        # å®šä¹‰è¾“å‡ºå±‚,ç”¨äºå°†ç¼–ç ç»“æœè¾“å‡ºä¸ºæ—¶é—´åºåˆ—é¢„æµ‹ç»“æœ
        self.linear_out = nn.Linear(GRU_n_dim, 1)
        
        # åˆå§‹åŒ–è¾“å‡ºå±‚çš„æƒé‡,ä½¿ç”¨ Xavier æ ‡å‡†æ­£æ€åˆ†å¸ƒ
        nn.init.xavier_normal_(self.linear_out.weight.data)
        
        # åˆå§‹åŒ–è¾“å‡ºå±‚çš„åç½®ä¸º 0.1
        self.linear_out.bias.data.fill_(0.1)

    def _compute_sampling_threshold(self, batches_seen):
        """
        åŠ¨æ€è®¡ç®—é‡‡æ ·é˜ˆå€¼,éšç€è®­ç»ƒè¿›è¡ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥
        :param batches_seen: å·²è§æ‰¹æ¬¡
        :return: é‡‡æ ·é˜ˆå€¼
        """
        return self.cl_decay_steps / (
            self.cl_decay_steps + np.exp(batches_seen / self.cl_decay_steps))

    def encoder(self, inputs, adj, head):
        """
        ç¼–ç å™¨çš„å‰å‘ä¼ æ’­
        é€šè¿‡éå†æ—¶é—´æ­¥å’Œé‚»æ¥çŸ©é˜µ,é€æ­¥ç¼–ç è¾“å…¥çš„æ—¶é—´åºåˆ—æ•°æ®,å¹¶è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€
        :param inputs: è¾“å…¥æ•°æ®ï¼ˆå½¢çŠ¶ä¸º B x N x T)
        :param adj: é‚»æ¥çŸ©é˜µï¼ˆè¡¨ç¤ºå›¾ç»“æ„ï¼‰
        :param head: å›¾å­¦ä¹ å™¨çš„å¤´ç¼–å·
        :return: ç¼–ç åçš„éšè—çŠ¶æ€å¼ é‡
        """
        encoder_hidden_state = None  # åˆå§‹åŒ–éšè—çŠ¶æ€
        encoder_hidden_state_tensor = torch.zeros(inputs.shape).to(self.device)  # åˆå§‹åŒ–å­˜å‚¨éšè—çŠ¶æ€çš„å¼ é‡

        # å¯¹æ¯ä¸€ä¸ªæ—¶é—´æ­¥æ‰§è¡Œç¼–ç æ“ä½œ
        for t in range(self.len_sequence):
            # è°ƒç”¨å¯¹åº”å¤´ç¼–å·çš„ç¼–ç å™¨æ¨¡å‹è¿›è¡Œç¼–ç ,æ›´æ–°éšè—çŠ¶æ€
            # éšè—çŠ¶æ€h_tç”±å½“å‰è¾“å…¥x_tå’Œä¸Šä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€h_(t-1)å…±åŒå†³å®š
            _, encoder_hidden_state = self.encoder_model[head](inputs[..., t], adj, encoder_hidden_state)
            # å°†ç¼–ç åçš„éšè—çŠ¶æ€ä¿å­˜åˆ°å¼ é‡ä¸­
            encoder_hidden_state_tensor[..., t] = encoder_hidden_state[-1, ...].reshape(-1, self.num_nodes, self.GRU_n_dim)

        return encoder_hidden_state_tensor  # è¿”å›ç¼–ç åçš„éšè—çŠ¶æ€å¼ é‡

    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­
        :param inputs: è¾“å…¥æ•°æ® [B,N,T]
        :return: æ¦‚ç‡å’Œè¾“å‡º
        """
        B = inputs.shape[0]  # è·å–è¾“å…¥æ‰¹é‡å¤§å°
        input_projected = self.linear1(inputs.unsqueeze(-1))  # é€šè¿‡çº¿æ€§å±‚å¯¹è¾“å…¥è¿›è¡ŒæŠ•å½±    [B, N, T, GRU_n_dim]
        input_projected = input_projected.permute(0, 1, 3, 2)  # è°ƒæ•´ç»´åº¦é¡ºåºä»¥é€‚åº”æ¨¡å‹çš„è¾“å…¥æ ¼å¼   [B, N, GRU_n_dim, T]

        # é€šè¿‡å›¾å­¦ä¹ å™¨è®¡ç®—èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»æ¦‚ç‡ï¼ˆå›¾ä¸­çš„ Relation Inferenceï¼‰
        probs = self.graph_learner(inputs)    # [B, head, N, N]

        # æ„å»ºæ©ç çŸ©é˜µ,ç”¨äºå»é™¤å›¾ä¸­èŠ‚ç‚¹ä¸è‡ªå·±çš„è¿æ¥
        # ç”Ÿæˆä¸€ä¸ªå¯¹è§’çº¿ä¸ºTrue,å…¶ä»–éƒ¨åˆ†ä¸ºfalseçš„æ©ç çŸ©é˜µmask_loc,ç”¨äºå¿½ç•¥è‡ªå·±è¿æ¥(èŠ‚ç‚¹ä¸è‡ªå·±çš„è¿æ¥)
        mask_loc = torch.eye(self.num_nodes, dtype=bool).to(self.device)
        # å»é™¤å¯¹è§’çº¿çš„å…ƒç´ ,è·å–èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥æ¦‚ç‡
        probs_reshaped = probs.masked_select(~mask_loc).view(B, self.head, self.num_nodes * (self.num_nodes - 1)).to(self.device)
        probs_reshaped = probs_reshaped.permute(0, 2, 1)

        # å¯¹è¿æ¥æ¦‚ç‡åº”ç”¨ softmax,å°†æƒé‡å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ,ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ¦‚ç‡ä¹‹å’Œä¸º1
        prob = F.softmax(probs_reshaped, -1)

        # é€šè¿‡ Gumbel-softmax è¿›è¡Œé‡‡æ ·,ç¡®å®šæœ€ç»ˆçš„å›¾ç»“æ„
        edges = gumbel_softmax(torch.log(prob + 1e-5), tau=self.temperature, hard=True).to(self.device)
        # è®¡ç®—å‡ºçš„å›¾å…³ç³»é€šè¿‡Gumbel-softmaxè¿›è¡Œé‡‡æ ·,é‡‡æ ·åçš„ç»“æ„å˜ä¸ºæ½œåœ¨å˜é‡z,ç”¨ä»¥ç¡®å®šèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
        # **å¯¹åº”å›¾ä¸­çš„sampling**

        # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µåˆ—è¡¨,ç”¨äºå­˜å‚¨æ¯ä¸ªå¤´çš„é‚»æ¥çŸ©é˜µ
        adj_list = torch.ones(self.head, B, self.num_nodes, self.num_nodes).to(self.device)
        # æ„å»ºæ©ç ,ç”¨äºå¿½ç•¥å¯¹è§’å…ƒç´ ï¼ˆèŠ‚ç‚¹ä¸è‡ªèº«çš„è¿æ¥ï¼‰
        mask = ~torch.eye(self.num_nodes, dtype=bool).unsqueeze(0).unsqueeze(0).to(self.device)
        mask = mask.repeat(self.head, B, 1, 1).to(self.device)

        # å°†é‡‡æ ·å¾—åˆ°çš„è¾¹å¡«å……åˆ°é‚»æ¥çŸ©é˜µä¸­,å³å­¦ä¹ åˆ°çš„å›¾ç»“æ„(èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥)å¡«å……åˆ°é‚»æ¥çŸ©é˜µä¸­
        adj_list[mask] = edges.permute(2, 0, 1).flatten()
        # print(adj_list.shape)
        logging.info(f"GRELEN_gcné‚»æ¥çŸ©é˜µçš„å½¢çŠ¶ä¸º{adj_list.shape}")

        # åˆå§‹åŒ–è¾“å‡ºçŠ¶æ€å¼ é‡,ç”¨äºå­˜å‚¨ç¼–ç ç»“æœ
        state_for_output = torch.zeros(input_projected.shape).to(self.device)
        state_for_output = (state_for_output.unsqueeze(0)).repeat(self.head - 1, 1, 1, 1, 1)

        # å¯¹æ¯ä¸ªå¤´è¿›è¡Œç¼–ç ,åœ¨å¤´éƒ¨ç»´åº¦ä¸Šå¾ªç¯,å¤„ç†ä¸åŒçš„å›¾ç»“æ„
        for head in range(self.head - 1):
            # è°ƒç”¨ç¼–ç å™¨è¿›è¡Œå‰å‘ä¼ æ’­,ä»è¾“å…¥æ•°æ®ä¸­æå–æ—¶åºç‰¹å¾,å¹¶åœ¨æ¯ä¸ªheadç”Ÿæˆå¯¹åº”çš„éšè—çŠ¶æ€h,å¹¶å°†ç¼–ç ç»“æœå­˜å‚¨åˆ° state_for_output ä¸­
            state_for_output[head, ...] = self.encoder(input_projected, adj_list[head + 1, ...], head)

        # state_for_output2å’Œoutputå¯¹åº”å›¾ä¸­çš„Decoderå’ŒSeries Reconstructionéƒ¨åˆ†
        # å°†ç¼–ç åçš„æ—¶åºç‰¹å¾é€šè¿‡çº¿æ€§å±‚è¿›è¡Œé‡æ„,ç”Ÿæˆé¢„æµ‹è¾“å‡º outputã€‚æ­¤è¿‡ç¨‹å¯¹åº”å›¾ä¸­çš„è§£ç å™¨å’Œåºåˆ—é‡æ„éƒ¨åˆ†,å®ƒè´Ÿè´£å°†ç¼–ç åçš„ç‰¹å¾ï¼ˆæˆ–éšè—çŠ¶æ€ï¼‰è¿˜åŸä¸ºé¢„æµ‹çš„æ—¶é—´åºåˆ—ã€‚
        # å¯¹æ‰€æœ‰å¤´çš„ç¼–ç ç»“æœå–å¹³å‡å€¼,å¹¶è°ƒæ•´ç»´åº¦
        state_for_output2 = torch.mean(state_for_output, 0).permute(0, 1, 3, 2)
        # é€šè¿‡çº¿æ€§å±‚linear_outç”Ÿæˆæœ€ç»ˆçš„æ—¶é—´åºåˆ—é¢„æµ‹ç»“æœ
        output = self.linear_out(state_for_output2).squeeze(-1)[..., -1 - self.target_T:-1]
        # é€šè¿‡å¯¹å¤šå¤´ç¼–ç å™¨çš„è¾“å‡ºå–å¹³å‡,æ¨¡å‹å¯ä»¥ç»¼åˆä¸åŒå¤´éƒ¨æ•è·çš„ç‰¹å¾,æå‡è¡¨ç¤ºçš„å¤šæ ·æ€§å’Œå®Œæ•´æ€§,å‡å°‘å•ä¸ªå¤´éƒ¨å¯èƒ½çš„è¯¯å·®,å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§

        # probè¡¨ç¤ºå›¾ç»“æ„çš„å­¦ä¹ ç»“æœ(å„èŠ‚ç‚¹çš„å…³ç³»),outputæ˜¯åŸºäºå›¾ç»“æ„è¿›è¡Œé¢„æµ‹çš„æ—¶é—´åºåˆ—
        # prob:èŠ‚ç‚¹é—´çš„ç›¸å…³æ€§æ¦‚ç‡çŸ©é˜µ,å½¢çŠ¶ä¸º[B,N(N-1),H];output:æ¨¡å‹çš„é¢„æµ‹è¾“å‡º,å½¢çŠ¶ä¸º[B,N,target_T]
        return prob, output  # è¿”å›å›¾ç»“æ„çš„æ¦‚ç‡å’Œé¢„æµ‹çš„æ—¶é—´åºåˆ—ç»“æœ
